{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Question-Answering Chatbot with Gradio\n",
    "\n",
    "In this exercise, we will use [LangChain](https://python.langchain.com/docs/get_started/introduction.html) and OpenAI's ChatGPT API to build a chatbot that can answer questions using information from a set of source documents.\n",
    "\n",
    "LangChain is a framework for building apps using language models. There are many off-the-shelf chains - sequences of calls to components, including other chains - that make it easy to get something going really quickly.\n",
    "\n",
    "#### Part 1: Chatbot Interface\n",
    "First let's build the interface using Gradio and get the skeleton of the chatbot in place. \n",
    "\n",
    "For the time being, we will use `send_chat_message()` to simulate the chatbot's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "\n",
    "def send_chat_message(prompt):\n",
    "    return random.choice([\"How are you?\", \"Have a great day!\", \"I'm very hungry\"])\n",
    "\n",
    "def respond(message, chat_history):\n",
    "    bot_message = send_chat_message(message)\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "with gr.Blocks(title = \"Datajam DocBot\") as demo:\n",
    "    with gr.Row():\n",
    "        gr.Markdown( # You can use Markdown for additional styling\n",
    "        \"\"\"\n",
    "        <h1 align='left'> <img src='file/assets/datajam-logo.png' height='50' width='129'> </h1>\n",
    "        <h1 align=\"center\"> DocBot ü§ñüí¨ </h1>\n",
    "        <p align='center' style='font-size: 18px;'> Meet ‚ú® DocBot ‚ú®, an intelligent chatbot that helps answer your deepest questions about the Unbounce universe. </p>\n",
    "        \"\"\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox(placeholder=\"Have a question? How can I help? Start the chat.\", show_label=False)\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: The AI Chatbot\n",
    "Now we can build the question-answering (QA) chatbot component using large language models (LLM). We won't be training our own model, instead we shall use OpenAI's [chat API](https://platform.openai.com/docs/guides/gpt) to query their pre-trained model.\n",
    "\n",
    "##### Using Environment Files\n",
    "In the last demo, we put our secret key in plaintext in the notebook itself. This is not very secure. If we wanted to share this notebook with someone else, we would always have to remember to remove the secret key.\n",
    "\n",
    "An alternative is to use environment files and simply use the notebook to read the values from that file. To do this,\n",
    "1. Create a new blank file in the same directory and call it `.env`\n",
    "2. Write `OPENAI_API_KEY=<your secret key>`\n",
    "3. Now we can read these environment variables using `load_dotenv()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API keys\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chatbot Pipeline\n",
    "The pipeline for converting raw unstructured data into a QA chain looks like this:\n",
    "\n",
    "1. **Loading**: First we need to load our data. Unstructured data can be loaded from many sources. The LangChain integration hub has the full set of loaders. Each loader returns data as a LangChain Document.\n",
    "2. **Splitting**: Text splitters break documents into chunks of specified size\n",
    "3. **Storage**: Storage (e.g., often a [vectorstore](https://python.langchain.com/docs/modules/data_connection/vectorstores/)) will house and often embed the chunks (read more about embeddings [here](https://www.pinecone.io/learn/vector-embeddings/))\n",
    "4. **Retrieval**: The app retrieves splits from storage (e.g., often with similar embeddings to the input question)\n",
    "5. **Generation**: An LLM produces an answer using a prompt that includes the question and the retrieved data\n",
    "6. **Conversation (Extension)**: Hold a multi-turn conversation by adding Memory to your QA chain.\n",
    "\n",
    "##### Step 1: Load\n",
    "The [Unstructured PDF Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf#using-unstructured) reads in PDFs and returns a LangChain Document object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "DATA_DIRECTORY = \"../data\"\n",
    "path = os.path.join(DATA_DIRECTORY, \"content_library.pdf\")\n",
    "loader = UnstructuredPDFLoader(path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2-4: Split, Store, Retrieve\n",
    "The rest of the pipeline can be wrapped into a single object: `VectorstoreIndexCreator` which splits, embeds, stores and retrieves data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_kwargs={\"persist_directory\": DATA_DIRECTORY} # Pass in the directory where embedded vectors will be stored\n",
    ").from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5-6: Generate and Converse\n",
    "Chains are *the* main feature of LangChain, enabling us to build complex applications by linking up different components. The components can be language models, other chains, document retrievers, prompts, utils, etc. There are a number of pre-built chains to help you get started.\n",
    "\n",
    "For this workshop, let's use a [Conversational Retrieval Chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html). It answers new questions using chat history (a list of previous messages) and source documents.\n",
    "\n",
    "The three main components of this chain are:\n",
    "\n",
    "1. Using the chat history and the new question to create a ‚Äústandalone question‚Äù: This way, the question can be passed into the retrieval step to fetch relevant documents. For eg., consider a follow-up question like \"Where can I find it?\". If only the new question was passed in, then language model would have no idea what 'it' is. If the whole conversation was passed in, there may be unnecessary information that would distract from retrieval.\n",
    "\n",
    "2. Retrieving relevant documents using this new question.\n",
    "\n",
    "3. Obtaining the final response: The retrieved documents are passed to an LLM along with the new question and it uses them to generate an answer.\n",
    "\n",
    "For both steps 1 and 3, we'll use an LLM, but we can specify different LLMs and different parameters. Here, we use the [ChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html) model and GPT-3.5 to generate the \"standalone question\" and GPT-4 to generate the response. You can pass in optional parameters for the [OpenAI API](https://platform.openai.com/docs/api-reference/chat/create) calls here. For example, we use a temperature value of 0.9; higher values like this will make the output more random, while lower values will make it more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ChatOpenAI(model=\"gpt-4\", temperature=0.9), # LLM used to generate the response\n",
    "    retriever=index.vectorstore.as_retriever(search_kwargs={\"k\": 1}), # Vector store where the chunked and embedded document is stored\n",
    "    condense_question_llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo'), # LLM used to generate the new question\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put this all together into a new method `send_chat_message()` that calls the chain with the user question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_chat_message(prompt, chat_history, chain):\n",
    "    \"\"\"\n",
    "    Converts Gradio's chat history format to LangChain's expected \n",
    "    format and queries the chat engine for a response.\n",
    "    \"\"\"\n",
    "    langchain_history = [(msg[0], msg[1]) for msg in chat_history]\n",
    "\n",
    "    result = chain({\"question\": prompt, \"chat_history\": langchain_history})\n",
    "    return result[\"answer\"]\n",
    "\n",
    "def respond(message, chat_history):\n",
    "    bot_message = send_chat_message(message, chat_history, chain)\n",
    "    chat_history.append((message, bot_message))\n",
    "    return \"\", chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(title = \"Datajam DocBot\") as demo:\n",
    "    with gr.Row():\n",
    "        gr.Markdown(\n",
    "        \"\"\"\n",
    "        <h1 align='left'> <img src='file/assets/datajam-logo.png' height='50' width='129'> </h1>\n",
    "        <h1 align=\"center\"> DocBot ü§ñüí¨ </h1>\n",
    "        <p align='center' style='font-size: 18px;'> Meet ‚ú® DocBot ‚ú®, an intelligent chatbot that helps answer your deepest questions about the Unbounce universe. </p>\n",
    "        \"\"\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox(placeholder=\"Have a question? How can I help? Start the chat.\", show_label=False)\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    gr.Examples( # UI to display some sample questions\n",
    "        examples=[\n",
    "            \"How do I add my domain to Unbounce?\",\n",
    "            \"What is a compelling call to action?\",\n",
    "            \"How do I add a user to my account?\",\n",
    "            \"How does Smart Traffic compare to A/B testing?\",\n",
    "            \"What are the options within Account Management?\",\n",
    "            \"What is a landing page?\"\n",
    "        ],\n",
    "        inputs=msg\n",
    "    )\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit ('3.10.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afcddcb8b5b7b9fe5a2ef76cc89bcf24cd8f6b761938ac67848103e97e668db6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
